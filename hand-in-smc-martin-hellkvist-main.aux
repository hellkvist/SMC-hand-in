\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {paragraph}{(a)}{1}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{(b)}{1}{section*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces For three different values of $N$ we see different variance in the estimate of Z. As $N$ increases, the variance decreases. The average of each histogram is close to the true value $Z=\sqrt  {2\pi }$. The histogram was generated with 1000 simulations for each N.}}{2}{figure.1}}
\newlabel{fig:1bhist}{{1}{2}{For three different values of $N$ we see different variance in the estimate of Z. As $N$ increases, the variance decreases. The average of each histogram is close to the true value $Z=\sqrt {2\pi }$. The histogram was generated with 1000 simulations for each N}{figure.1}{}}
\@writefile{toc}{\contentsline {paragraph}{(c)}{2}{section*.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Task 1c. The distribution of the estimated normalization was skewed due to thin tails of the proposal.}}{3}{figure.2}}
\newlabel{fig:1chist1}{{2}{3}{Task 1c. The distribution of the estimated normalization was skewed due to thin tails of the proposal}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Task 1c. When the tails of the proposal were heavier, the estimates clearly clearly illustrated the unbiasedness of the IS.}}{3}{figure.3}}
\newlabel{fig:1chist2}{{3}{3}{Task 1c. When the tails of the proposal were heavier, the estimates clearly clearly illustrated the unbiasedness of the IS}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces 2c. Simulation results for the bootstrap particle filter. MAD is the mean absolute difference between the BPF estimated trajectory and the Kalman trajectory. The variance denotes the variance of the MAD over the iterations.}}{4}{table.1}}
\newlabel{tab:2c}{{1}{4}{2c. Simulation results for the bootstrap particle filter. MAD is the mean absolute difference between the BPF estimated trajectory and the Kalman trajectory. The variance denotes the variance of the MAD over the iterations}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces 2c. Variance over particles per iteration of the BPF state estimates compared to the Kalman variance.}}{4}{figure.4}}
\newlabel{fig:2_c}{{4}{4}{2c. Variance over particles per iteration of the BPF state estimates compared to the Kalman variance}{figure.4}{}}
\@writefile{toc}{\contentsline {paragraph}{(a)}{4}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{(b)}{4}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{(c)}{4}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{(d)}{4}{section*.9}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces 2d. The mean average difference between the Fully Adapted Particle filter's state trajectory and the Kalman filter's state trajectory.}}{5}{table.2}}
\newlabel{tab:2d}{{2}{5}{2d. The mean average difference between the Fully Adapted Particle filter's state trajectory and the Kalman filter's state trajectory}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces 2e. Particle genealogy from time 1950 to 1999 for the fully adaptive particle filter. }}{5}{figure.5}}
\newlabel{fig:2_e}{{5}{5}{2e. Particle genealogy from time 1950 to 1999 for the fully adaptive particle filter}{figure.5}{}}
\@writefile{toc}{\contentsline {paragraph}{(e)}{5}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{(f)}{5}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{(g)}{5}{section*.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces 2f. The genealogy for the final 100 particles.}}{6}{figure.6}}
\newlabel{fig:2_f}{{6}{6}{2f. The genealogy for the final 100 particles}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 2f. The early genealogy for the final 100 particles.}}{6}{figure.7}}
\newlabel{fig:2_f_start}{{7}{6}{2f. The early genealogy for the final 100 particles}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces 2g. (Top) The early genealogy for the final 100 particles using systematic resampling with ESS trigger at $ N_{\text  {thresh}} = 50 $. (Bottom) The ESS in each iteration from 0 to 100.}}{6}{figure.8}}
\newlabel{fig:2_g}{{8}{6}{2g. (Top) The early genealogy for the final 100 particles using systematic resampling with ESS trigger at $ N_{\text {thresh}} = 50 $. (Bottom) The ESS in each iteration from 0 to 100}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces 3a. Boxplot of the 10 log-likelihood estimates for each value of $\phi $. Note that the spacing of the phi-axis is smaller between 0.95 and 1, than it is before.}}{7}{figure.9}}
\newlabel{fig:3_a_new}{{9}{7}{3a. Boxplot of the 10 log-likelihood estimates for each value of $\phi $. Note that the spacing of the phi-axis is smaller between 0.95 and 1, than it is before}{figure.9}{}}
\@writefile{toc}{\contentsline {paragraph}{(a)}{7}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{(b)}{7}{section*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces 3b. (Top and middle) The evolution of $\beta ^2$ and $ \sigma ^2 $ over the PMH iterations. (Bottom) Histograms of the parameters over the last 8000 iterations.}}{8}{figure.10}}
\newlabel{fig:3_b}{{10}{8}{3b. (Top and middle) The evolution of $\beta ^2$ and $ \sigma ^2 $ over the PMH iterations. (Bottom) Histograms of the parameters over the last 8000 iterations}{figure.10}{}}
\@writefile{toc}{\contentsline {paragraph}{(c)}{8}{section*.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces 4a. (Top left) The particles at initialization were sampled from the standard uniform distribution. (Top right) The particles after 10 iterations had all been moved into several clusters. (Bottom left) The particles after 30 iterations were a lot more concentrated on $x_2=0.15$ while more spread out for $x_1$, some sinusoidal effect is seen over $x_2$ having a cluster between 0.4 and 0.6. (Bottom right) After 80 iterations almost no effect of the sinusoidal functions were visible as they were more overshadowed by the exponential.}}{9}{figure.11}}
\newlabel{fig:4_a}{{11}{9}{4a. (Top left) The particles at initialization were sampled from the standard uniform distribution. (Top right) The particles after 10 iterations had all been moved into several clusters. (Bottom left) The particles after 30 iterations were a lot more concentrated on $x_2=0.15$ while more spread out for $x_1$, some sinusoidal effect is seen over $x_2$ having a cluster between 0.4 and 0.6. (Bottom right) After 80 iterations almost no effect of the sinusoidal functions were visible as they were more overshadowed by the exponential}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces 4b. The number of iterations between resampling is increasing with the iteration number, while the average for these 150 iterations is 19. }}{9}{figure.12}}
\newlabel{fig:4_b}{{12}{9}{4b. The number of iterations between resampling is increasing with the iteration number, while the average for these 150 iterations is 19}{figure.12}{}}
